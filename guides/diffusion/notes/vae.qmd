---
title: "VAE 基础推导"
subtitle: "从最大似然到变分下界（ELBO）的完整逻辑"
description: "从前向噪声到反向回归目标，理解为什么只需学习均值。"
date: 2025-10-14
categories:
  - Diffusion
  - VAE
  - Generative Models
tags:
  - ELBO
  - Variational Inference
  - Machine Learning Theory
image: "../../images/diffusion-cover.png"  # 可替换封面；否则用默认占位图
format:
  html:
    toc: true
    toc-depth: 3
    theme: flatly
---

# VAE：从最大似然到 ELBO



## 🎯 从极大似然出发

我们希望估计某个分布 $p(x)$，用参数化模型 $p_\theta(x)$ 来近似，并通过极大似然估计优化参数：

$$
\max_\theta \ \mathbb{E}_{x\sim p^*}[\log p_\theta(x)].
$$

但在高维空间上直接建模 $p_\theta(x)$ 通常非常困难，因此我们引入**潜变量** $z$，将复杂分布分解为条件形式：

$$
p_\theta(x)=\int p_\theta(x|z)p(z)\,dz,
$$

其中 $p_\theta(x|z)$ 称为 **decoder（生成器）**，$p(z)$ 是先验（通常取标准正态）。

---

## 🧩 引入可计算的近似分布

真实的后验 $p_\theta(z|x)$ 无法直接计算，于是定义可计算的近似分布：

$$
q_\phi(z|x) \approx p_\theta(z|x),
$$

称为 **encoder（编码器）**。  
两者通常使用神经网络参数化：

$$
q_\phi(z|x)=\mathcal N(z|\mu_\phi(x),\mathrm{diag}(\sigma_\phi^2(x))), \quad
p_\theta(x|z)=\mathcal N(x|f_\theta(z),\sigma_{\text{dec}}^2 I).
$$

---

## 🧮 从最大似然到 ELBO

直接优化 $\log p_\theta(x)$ 不可行，因为积分项难计算：

$$
p_\theta(x)=\int p_\theta(x,z)\,dz.
$$

于是我们引入 $q_\phi(z|x)$ 并重写：

$$
\log p_\theta(x)
= \log\int q_\phi(z|x)\frac{p_\theta(x,z)}{q_\phi(z|x)}\,dz
= \log \mathbb{E}_{q_\phi}\!\left[\frac{p_\theta(x,z)}{q_\phi(z|x)}\right].
$$

应用 Jensen 不等式（$\log \mathbb{E}[\cdot] \ge \mathbb{E}[\log\cdot]$）得到：

$$
\log p_\theta(x)
\ge \mathbb{E}_{q_\phi}\!\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]
=: \mathcal L(\theta,\phi;x),
$$

这就是著名的 **Evidence Lower Bound (ELBO)**。

---

## 🧠 ELBO 的结构与意义

展开 $p_\theta(x,z)=p_\theta(x|z)p(z)$，有：

$$
\mathcal L(\theta,\phi;x)
= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
- \mathrm{KL}(q_\phi(z|x)\|p(z)).
$$

- **重建项**：衡量生成器 $p_\theta(x|z)$ 对输入 $x$ 的复现能力；
- **正则项**：约束编码器 $q_\phi(z|x)$ 贴近先验 $p(z)$，防止潜变量退化。

等价关系：

$$
\log p_\theta(x)
= \mathcal L(\theta,\phi;x)
+ \mathrm{KL}(q_\phi(z|x)\|p_\theta(z|x)),
$$

因此 ELBO 始终是 $\log p_\theta(x)$ 的下界。

---

## ⚙️ 梯度与可微优化

**对 $\theta$（生成器）求导：**

$$
\nabla_\theta \mathcal L
= \mathbb{E}_{q_\phi(z|x)}[\nabla_\theta \log p_\theta(x|z)].
$$

可用 Monte Carlo 近似实现。

**对 $\phi$（编码器）求导：**

若能重参数化 $z=g_\phi(x,\varepsilon)$ 且 $\varepsilon\sim p(\varepsilon)$：

$$
z=\mu_\phi(x)+\sigma_\phi(x)\odot\varepsilon,\quad \varepsilon\sim\mathcal N(0,I),
$$

则：

$$
\nabla_\phi \mathbb{E}_{q_\phi}[\log p_\theta(x|z)]
= \mathbb{E}_\varepsilon[\nabla_\phi \log p_\theta(x|g_\phi(x,\varepsilon))].
$$

这就是 **重参数化技巧**，可显著降低方差。  
对于离散 $z$，则可使用 REINFORCE 或 Gumbel–Softmax。

---

## 📋 本节速览

- 目标：极大似然估计 → 引入潜变量 $z$  
- 核心公式：  
  $\mathcal L = \mathbb{E}_{q_\phi}[\log p_\theta(x|z)] - \mathrm{KL}(q_\phi\|p)$  
- 关系：  
  $\log p_\theta(x) = \mathcal L + \mathrm{KL}(q_\phi\|p_\theta(\cdot|x))$  
- 实现：  
  重参数化采样、解析 KL、KL 退火 / $\beta$-VAE 防坍缩  

---

**推荐阅读：**  
📖 *arXiv:2406.08929* —— 一份以直觉为主的 Diffusion 入门材料。  
先建立整体框架，再看推导细节，会更高效。  

#生成模型 #VAE #Diffusion #变分推断
