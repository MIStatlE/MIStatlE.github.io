[
  {
    "objectID": "papers/2025-10-08-paper-template.html",
    "href": "papers/2025-10-08-paper-template.html",
    "title": "Self-Concordant Barriers Revisited",
    "section": "",
    "text": "Tip\n\n\n\nTL;DR 以自洽函数为桥梁，统一了内点法的复杂度与几何直觉。"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#key-ideas",
    "href": "papers/2025-10-08-paper-template.html#key-ideas",
    "title": "Self-Concordant Barriers Revisited",
    "section": "Key Ideas",
    "text": "Key Ideas\n\nBarrier 的自洽性定义与局部二次近似…\nNewton decrement 与路径跟随…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#why-it-matters",
    "href": "papers/2025-10-08-paper-template.html#why-it-matters",
    "title": "Self-Concordant Barriers Revisited",
    "section": "Why it matters",
    "text": "Why it matters\n\n将凸优化的复杂度理论从“算法技巧”上升到“几何结构”…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#my-notes",
    "href": "papers/2025-10-08-paper-template.html#my-notes",
    "title": "Self-Concordant Barriers Revisited",
    "section": "My notes",
    "text": "My notes\n\n直观图示…\n延伸阅读：…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#bibtex",
    "href": "papers/2025-10-08-paper-template.html#bibtex",
    "title": "Self-Concordant Barriers Revisited",
    "section": "BibTeX",
    "text": "BibTeX\n```bibtex @article{nesterov1994interior, … }"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PhD student / Researcher in Shanghai\nInterests: Probability, Machine Learning, Diffusion Models\nI write about ideas-in-progress — learning in public."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "PhD student / Researcher in Shanghai\nInterests: Probability, Machine Learning, Diffusion Models\nI write about ideas-in-progress — learning in public."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n\nGitHub: https://github.com/MIStatlE"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html",
    "href": "guides/diffusion/notes/vae.html",
    "title": "VAE 基础推导",
    "section": "",
    "text": "我们希望估计某个分布 p(x)，用参数化模型 p_\\theta(x) 来近似，并通过极大似然估计优化参数：\n\n\\max_\\theta \\ \\mathbb{E}_{x\\sim p^*}[\\log p_\\theta(x)].\n\n但在高维空间上直接建模 p_\\theta(x) 通常非常困难，因此我们引入潜变量 z，将复杂分布分解为条件形式：\n\np_\\theta(x)=\\int p_\\theta(x|z)p(z)\\,dz,\n\n其中 p_\\theta(x|z) 称为 decoder（生成器），p(z) 是先验（通常取标准正态）。\n\n\n\n\n真实的后验 p_\\theta(z|x) 无法直接计算，于是定义可计算的近似分布：\n\nq_\\phi(z|x) \\approx p_\\theta(z|x),\n\n称为 encoder（编码器）。\n两者通常使用神经网络参数化：\n\nq_\\phi(z|x)=\\mathcal N(z|\\mu_\\phi(x),\\mathrm{diag}(\\sigma_\\phi^2(x))), \\quad\np_\\theta(x|z)=\\mathcal N(x|f_\\theta(z),\\sigma_{\\text{dec}}^2 I).\n\n\n\n\n\n直接优化 \\log p_\\theta(x) 不可行，因为积分项难计算：\n\np_\\theta(x)=\\int p_\\theta(x,z)\\,dz.\n\n于是我们引入 q_\\phi(z|x) 并重写：\n\n\\log p_\\theta(x)\n= \\log\\int q_\\phi(z|x)\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\,dz\n= \\log \\mathbb{E}_{q_\\phi}\\!\\left[\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right].\n\n应用 Jensen 不等式（\\log \\mathbb{E}[\\cdot] \\ge \\mathbb{E}[\\log\\cdot]）得到：\n\n\\log p_\\theta(x)\n\\ge \\mathbb{E}_{q_\\phi}\\!\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\n=: \\mathcal L(\\theta,\\phi;x),\n\n这就是著名的 Evidence Lower Bound (ELBO)。\n\n\n\n\n展开 p_\\theta(x,z)=p_\\theta(x|z)p(z)，有：\n\n\\mathcal L(\\theta,\\phi;x)\n= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\n- \\mathrm{KL}(q_\\phi(z|x)\\|p(z)).\n\n\n重建项：衡量生成器 p_\\theta(x|z) 对输入 x 的复现能力；\n正则项：约束编码器 q_\\phi(z|x) 贴近先验 p(z)，防止潜变量退化。\n\n等价关系：\n\n\\log p_\\theta(x)\n= \\mathcal L(\\theta,\\phi;x)\n+ \\mathrm{KL}(q_\\phi(z|x)\\|p_\\theta(z|x)),\n\n因此 ELBO 始终是 \\log p_\\theta(x) 的下界。\n\n推荐阅读：\n📖 arXiv:2406.08929 —— 一份以直觉为主的 Diffusion 入门材料。\n先建立整体框架，再看推导细节，会更高效。\n#生成模型 #VAE #Diffusion #变分推断"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#从极大似然出发",
    "href": "guides/diffusion/notes/vae.html#从极大似然出发",
    "title": "VAE 基础推导",
    "section": "",
    "text": "我们希望估计某个分布 p(x)，用参数化模型 p_\\theta(x) 来近似，并通过极大似然估计优化参数：\n\n\\max_\\theta \\ \\mathbb{E}_{x\\sim p^*}[\\log p_\\theta(x)].\n\n但在高维空间上直接建模 p_\\theta(x) 通常非常困难，因此我们引入潜变量 z，将复杂分布分解为条件形式：\n\np_\\theta(x)=\\int p_\\theta(x|z)p(z)\\,dz,\n\n其中 p_\\theta(x|z) 称为 decoder（生成器），p(z) 是先验（通常取标准正态）。"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#引入可计算的近似分布",
    "href": "guides/diffusion/notes/vae.html#引入可计算的近似分布",
    "title": "VAE 基础推导",
    "section": "",
    "text": "真实的后验 p_\\theta(z|x) 无法直接计算，于是定义可计算的近似分布：\n\nq_\\phi(z|x) \\approx p_\\theta(z|x),\n\n称为 encoder（编码器）。\n两者通常使用神经网络参数化：\n\nq_\\phi(z|x)=\\mathcal N(z|\\mu_\\phi(x),\\mathrm{diag}(\\sigma_\\phi^2(x))), \\quad\np_\\theta(x|z)=\\mathcal N(x|f_\\theta(z),\\sigma_{\\text{dec}}^2 I)."
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#从最大似然到-elbo",
    "href": "guides/diffusion/notes/vae.html#从最大似然到-elbo",
    "title": "VAE 基础推导",
    "section": "",
    "text": "直接优化 \\log p_\\theta(x) 不可行，因为积分项难计算：\n\np_\\theta(x)=\\int p_\\theta(x,z)\\,dz.\n\n于是我们引入 q_\\phi(z|x) 并重写：\n\n\\log p_\\theta(x)\n= \\log\\int q_\\phi(z|x)\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\,dz\n= \\log \\mathbb{E}_{q_\\phi}\\!\\left[\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right].\n\n应用 Jensen 不等式（\\log \\mathbb{E}[\\cdot] \\ge \\mathbb{E}[\\log\\cdot]）得到：\n\n\\log p_\\theta(x)\n\\ge \\mathbb{E}_{q_\\phi}\\!\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\n=: \\mathcal L(\\theta,\\phi;x),\n\n这就是著名的 Evidence Lower Bound (ELBO)。"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#elbo-的结构与意义",
    "href": "guides/diffusion/notes/vae.html#elbo-的结构与意义",
    "title": "VAE 基础推导",
    "section": "",
    "text": "展开 p_\\theta(x,z)=p_\\theta(x|z)p(z)，有：\n\n\\mathcal L(\\theta,\\phi;x)\n= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\n- \\mathrm{KL}(q_\\phi(z|x)\\|p(z)).\n\n\n重建项：衡量生成器 p_\\theta(x|z) 对输入 x 的复现能力；\n正则项：约束编码器 q_\\phi(z|x) 贴近先验 p(z)，防止潜变量退化。\n\n等价关系：\n\n\\log p_\\theta(x)\n= \\mathcal L(\\theta,\\phi;x)\n+ \\mathrm{KL}(q_\\phi(z|x)\\|p_\\theta(z|x)),\n\n因此 ELBO 始终是 \\log p_\\theta(x) 的下界。\n\n推荐阅读：\n📖 arXiv:2406.08929 —— 一份以直觉为主的 Diffusion 入门材料。\n先建立整体框架，再看推导细节，会更高效。\n#生成模型 #VAE #Diffusion #变分推断"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides Overview",
    "section": "",
    "text": "Guides Overview\nStart with Probability & Stats or Optimization.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "guides/book/index.html",
    "href": "guides/book/index.html",
    "title": "RL / Diffusion Notebook",
    "section": "",
    "text": "这本笔记旨在系统整理我在强化学习与扩散模型研究中的思考与推导。\n它并不是教程，而是一种 理解路径（path to clarity）——\n把复杂的理论、模糊的直觉、严谨的公式，用一种兼具推理与感性的方式串联起来。\n\n\n\n\nPart I. Reinforcement Learning\n从最简单的决策过程开始，构建期望回报、策略梯度与信用分配的统一理解。\nPart II. Diffusion Models\n从随机过程视角理解扩散模型，逐步推导得分匹配与反向 SDE。\n\n\n\n\n\n\n左侧为全书章节导航，可自由跳转。\n\n右侧为当前页小节目录，帮助快速定位。\n\n数学公式使用 KaTeX 渲染，可直接复制。\n\n代码片段可展开、复制。\n\n\n\n\n\n\n\n\n符号\n含义\n\n\n\n\n( s_t, a_t, r_t )\n状态、动作、奖励\n\n\n( _(a\ns) )\n\n\n( J() )\n策略目标函数\n\n\n( p_t(x) )\n扩散过程在时间 (t) 的分布\n\n\n\n\n\n\n\n\n“我希望这本书既是科研笔记，也是一种自我认知的延伸。”\n— MIStatlE"
  },
  {
    "objectID": "guides/book/index.html#结构概览",
    "href": "guides/book/index.html#结构概览",
    "title": "RL / Diffusion Notebook",
    "section": "",
    "text": "Part I. Reinforcement Learning\n从最简单的决策过程开始，构建期望回报、策略梯度与信用分配的统一理解。\nPart II. Diffusion Models\n从随机过程视角理解扩散模型，逐步推导得分匹配与反向 SDE。"
  },
  {
    "objectID": "guides/book/index.html#阅读方式",
    "href": "guides/book/index.html#阅读方式",
    "title": "RL / Diffusion Notebook",
    "section": "",
    "text": "左侧为全书章节导航，可自由跳转。\n\n右侧为当前页小节目录，帮助快速定位。\n\n数学公式使用 KaTeX 渲染，可直接复制。\n\n代码片段可展开、复制。"
  },
  {
    "objectID": "guides/book/index.html#符号约定",
    "href": "guides/book/index.html#符号约定",
    "title": "RL / Diffusion Notebook",
    "section": "",
    "text": "符号\n含义\n\n\n\n\n( s_t, a_t, r_t )\n状态、动作、奖励\n\n\n( _(a\ns) )\n\n\n( J() )\n策略目标函数\n\n\n( p_t(x) )\n扩散过程在时间 (t) 的分布"
  },
  {
    "objectID": "guides/book/index.html#关于作者",
    "href": "guides/book/index.html#关于作者",
    "title": "RL / Diffusion Notebook",
    "section": "",
    "text": "“我希望这本书既是科研笔记，也是一种自我认知的延伸。”\n— MIStatlE"
  },
  {
    "objectID": "guides/probability/index.html",
    "href": "guides/probability/index.html",
    "title": "Probability Theory",
    "section": "",
    "text": "自动汇总笔记\n本页面将自动展示所有位于 notes/*.qmd 下的概率论笔记。\n在 guides/probability/notes/ 文件夹中新增 .qmd 文件即可自动更新列表。\n\n\n\n\n\n\n\n\n\n\n连续映射定理与 Slutsky 定理\n\n\n\nprobability\n\nProbability\n\nAsymptotic Theory\n\nStatistical Inference\n\n\n\n理解连续变换与不同收敛类型之间的关系，是概率极限定理体系的核心。\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Tools & Workflows",
    "section": "",
    "text": "可复用的研究/工程工具与工作流，强调 可操作 与 效率。\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nQuarto 最佳实践：从 0 到“高信息密度”站点\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Welcome to my posts.\n\n\n\n\n\n\n\n\n\nStarting Over: Building MIStatlE Lab\n\n\n\nmeta\n\nsetup\n\n\n\nA simple Quarto starter with math support, post listings, and GitHub Pages auto-deploy.\n\n\n\n\n\nOct 7, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "community/index.html",
    "href": "community/index.html",
    "title": "Community",
    "section": "",
    "text": "Community\nEvents, announcements, and resources."
  },
  {
    "objectID": "reads/index.html",
    "href": "reads/index.html",
    "title": "Good Reads",
    "section": "",
    "text": "高信息密度外部资源：长文、书单、课程、视频与博客。\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hello, I’m MIStatlE 👋\nThis is my minimal Quarto website. Math works out of the box:\n\nInline math: \\int_0^1 x^2\\,dx = 1/3\nDisplay math:\n\n\n\\mathbb{P}(|S_n - n\\mu| \\ge t) \\le 2\\exp\\!\\left(-\\frac{t^2}{2\\sigma^2 n}\\right).\n\nUse the Posts section for articles and notes. Edit _quarto.yml to customize the navbar and theme."
  },
  {
    "objectID": "search.html",
    "href": "search.html",
    "title": "FRAIMEL",
    "section": "",
    "text": "Tip\n\n\n\nUse the navbar search (browser) or your site search engine."
  },
  {
    "objectID": "search.html#title-search",
    "href": "search.html#title-search",
    "title": "FRAIMEL",
    "section": "",
    "text": "Tip\n\n\n\nUse the navbar search (browser) or your site search engine."
  },
  {
    "objectID": "posts/2025-10-07-welcome.html",
    "href": "posts/2025-10-07-welcome.html",
    "title": "Starting Over: Building MIStatlE Lab",
    "section": "",
    "text": "This is a starter post. You can write math like e^{i\\pi}+1=0 and code blocks:\ndef add(a, b): return a + b\nprint(add(2, 3))\nTo publish, just push to main — GitHub Actions will render and deploy to gh-pages."
  },
  {
    "objectID": "tools/quarto-setup.html",
    "href": "tools/quarto-setup.html",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "",
    "text": "Markdown+数学+列表页+RSS，一体化…\n适合“利他型”学习空间：低摩擦、高输出。"
  },
  {
    "objectID": "tools/quarto-setup.html#why-quarto",
    "href": "tools/quarto-setup.html#why-quarto",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "",
    "text": "Markdown+数学+列表页+RSS，一体化…\n适合“利他型”学习空间：低摩擦、高输出。"
  },
  {
    "objectID": "tools/quarto-setup.html#setup",
    "href": "tools/quarto-setup.html#setup",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "Setup",
    "text": "Setup\n```bash quarto create-project MIStatlE-Lab quarto add quarto-ext/fontawesome"
  },
  {
    "objectID": "guides/probability/notes/slutsky.html",
    "href": "guides/probability/notes/slutsky.html",
    "title": "连续映射定理与 Slutsky 定理",
    "section": "",
    "text": "学习目标\n掌握随机变量收敛的三种形式（几乎处处、依概率、分布收敛），\n理解连续映射定理与 Slutsky 定理在渐近理论中的作用。\n\n\n\n\n在概率论与统计推断中，许多核心结论（如中心极限定理、渐近正态性、估计量一致性）都依赖于两个基本问题：\n\n当我们对随机变量做连续变换时，收敛性质是否仍然保持？\n当我们把不同类型的收敛（如分布收敛和概率收敛）结合在一起时，能否得到稳定的极限描述？\n\n这两点分别由 连续映射定理 与 Slutsky 定理 保证。\n\n\n\n\n\n定理.\n设函数 g 在集合 B 上连续，且 P(X\\in B)=1。则以下三种收敛关系均成立：\n\n若 X_n \\xrightarrow{a.s.} X，则 g(X_n) \\xrightarrow{a.s.} g(X)；\n\n若 X_n \\xrightarrow{p} X，则 g(X_n) \\xrightarrow{p} g(X)；\n\n若 X_n \\xrightarrow{d} X，则 g(X_n) \\xrightarrow{d} g(X)。\n\n\n\n\n\n连续性与收敛的相容性：\n\n几乎处处收敛： 在概率为 1 的集合上，X_n(\\omega)\\to X(\\omega)，而 g 在该点连续，故 g(X_n(\\omega))\\to g(X(\\omega))，即 g(X_n)\\xrightarrow{a.s.} g(X)。\n\n依概率收敛： 利用 子列刻画定理。任意子列 X_{n_k} 都可抽出子子列几乎处处收敛到 X；由上一步得到 g(X_{n_k}) 的对应子子列依概率收敛到 g(X)；因此 g(X_n)\\xrightarrow{p} g(X)。\n\n分布收敛： 使用 Portmanteau 定理。考察闭集与开集的原像\nC_t=\\{x: g(x)\\le t\\} 与 O_t=\\{x: g(x)&lt;t\\}，结合 g 的连续性与 P(X\\in B)=1，即可比较 P(g(X_n)\\le t) 与 P(g(X)\\le t)，得出 g(X_n)\\xrightarrow{d} g(X)。\n\n\n\n\n\n收敛类型在连续变换下是稳定的。\n例如，当 X_n \\to c 时：\n\n\\sin(X_n)\\to \\sin(c), \\quad\nX_n^2\\to c^2, \\quad\ne^{X_n}\\to e^c.\n\n这意味着即使我们对随机变量进行非线性变换（如平方、指数、激活函数等），\n收敛性质依然保持不变。\n\n\n\n\n\n\n定理.\n1. 若 X_n \\xrightarrow{d} c，则 X_n \\xrightarrow{p} c；反之亦然。\n2. 若 X_n \\xrightarrow{d} X 且 d(X_n,Y_n) \\xrightarrow{p} 0，则 Y_n \\xrightarrow{d} X。\n3. 若 X_n \\xrightarrow{d} X 且 Y_n \\xrightarrow{p} c，则\n\n\\begin{pmatrix}\nX_n, Y_n\n\\end{pmatrix}\n\\xrightarrow{d}\n\\begin{pmatrix}\nX, c\n\\end{pmatrix}.\n\n\n\n\n\n\n(1) 若 X_n \\xrightarrow{d} c，则其概率质量集中于 c 附近，即 \\Pr(|X_n-c|&gt;\\varepsilon)\\to 0；反之亦然。\n\n(2) 取有界连续函数 \\varphi，比较 \\E[\\varphi(Y_n)]-\\E[\\varphi(X)]，并拆为\n\\E[\\varphi(Y_n)-\\varphi(X_n)]+\\E[\\varphi(X_n)-\\varphi(X)]。\n前者由均匀连续性与 d(X_n,Y_n)\\xrightarrow{p}0 控制为 0，后者由 X_n\\xrightarrow{d}X 控制，故 Y_n\\xrightarrow{d}X。\n\n(3) 取二维连续函数 \\psi(x,y)，插入 \\psi(X_n,c)，结合 X_n\\xrightarrow{d}X 与 Y_n\\xrightarrow{p}c，即可得\n(X_n,Y_n)\\xrightarrow{d}(X,c)。\n\n\n\n\n\n由 连续映射定理 与 Slutsky 定理 可得以下常用结论：\n\nX_n + Y_n \\xrightarrow{d} X + c, \\quad\nX_n Y_n \\xrightarrow{d} cX.\n\n若 c \\neq 0，还可得到：\n\n\\frac{X_n}{Y_n} \\xrightarrow{d} \\frac{X}{c}.\n\n\n\n\n\n\n\n要点回顾：\n\n连续映射定理保证了“函数变换”下的收敛保持；\nSlutsky 定理则描述了“混合类型收敛”的稳定性；\n二者共同构成极限理论与渐近统计分析的技术基础。\n\n\n\n延伸阅读：\n📘 Probability and Measure — Billingsley, Ch. (Chapter 2–3)\n📗 Asymptotic Statistics — van der Vaart, A. W.\n📙 High-Dimensional Probability — Vershynin, R.\n推荐顺序：先掌握三类收敛，再阅读 Portmanteau 与 Slutsky 的证明。"
  },
  {
    "objectID": "guides/probability/notes/slutsky.html#背景与动机",
    "href": "guides/probability/notes/slutsky.html#背景与动机",
    "title": "连续映射定理与 Slutsky 定理",
    "section": "",
    "text": "在概率论与统计推断中，许多核心结论（如中心极限定理、渐近正态性、估计量一致性）都依赖于两个基本问题：\n\n当我们对随机变量做连续变换时，收敛性质是否仍然保持？\n当我们把不同类型的收敛（如分布收敛和概率收敛）结合在一起时，能否得到稳定的极限描述？\n\n这两点分别由 连续映射定理 与 Slutsky 定理 保证。"
  },
  {
    "objectID": "guides/probability/notes/slutsky.html#连续映射定理continuous-mapping-theorem",
    "href": "guides/probability/notes/slutsky.html#连续映射定理continuous-mapping-theorem",
    "title": "连续映射定理与 Slutsky 定理",
    "section": "",
    "text": "定理.\n设函数 g 在集合 B 上连续，且 P(X\\in B)=1。则以下三种收敛关系均成立：\n\n若 X_n \\xrightarrow{a.s.} X，则 g(X_n) \\xrightarrow{a.s.} g(X)；\n\n若 X_n \\xrightarrow{p} X，则 g(X_n) \\xrightarrow{p} g(X)；\n\n若 X_n \\xrightarrow{d} X，则 g(X_n) \\xrightarrow{d} g(X)。\n\n\n\n\n\n连续性与收敛的相容性：\n\n几乎处处收敛： 在概率为 1 的集合上，X_n(\\omega)\\to X(\\omega)，而 g 在该点连续，故 g(X_n(\\omega))\\to g(X(\\omega))，即 g(X_n)\\xrightarrow{a.s.} g(X)。\n\n依概率收敛： 利用 子列刻画定理。任意子列 X_{n_k} 都可抽出子子列几乎处处收敛到 X；由上一步得到 g(X_{n_k}) 的对应子子列依概率收敛到 g(X)；因此 g(X_n)\\xrightarrow{p} g(X)。\n\n分布收敛： 使用 Portmanteau 定理。考察闭集与开集的原像\nC_t=\\{x: g(x)\\le t\\} 与 O_t=\\{x: g(x)&lt;t\\}，结合 g 的连续性与 P(X\\in B)=1，即可比较 P(g(X_n)\\le t) 与 P(g(X)\\le t)，得出 g(X_n)\\xrightarrow{d} g(X)。\n\n\n\n\n\n收敛类型在连续变换下是稳定的。\n例如，当 X_n \\to c 时：\n\n\\sin(X_n)\\to \\sin(c), \\quad\nX_n^2\\to c^2, \\quad\ne^{X_n}\\to e^c.\n\n这意味着即使我们对随机变量进行非线性变换（如平方、指数、激活函数等），\n收敛性质依然保持不变。"
  },
  {
    "objectID": "guides/probability/notes/slutsky.html#slutsky-定理slutskys-theorem",
    "href": "guides/probability/notes/slutsky.html#slutsky-定理slutskys-theorem",
    "title": "连续映射定理与 Slutsky 定理",
    "section": "",
    "text": "定理.\n1. 若 X_n \\xrightarrow{d} c，则 X_n \\xrightarrow{p} c；反之亦然。\n2. 若 X_n \\xrightarrow{d} X 且 d(X_n,Y_n) \\xrightarrow{p} 0，则 Y_n \\xrightarrow{d} X。\n3. 若 X_n \\xrightarrow{d} X 且 Y_n \\xrightarrow{p} c，则\n\n\\begin{pmatrix}\nX_n, Y_n\n\\end{pmatrix}\n\\xrightarrow{d}\n\\begin{pmatrix}\nX, c\n\\end{pmatrix}.\n\n\n\n\n\n\n(1) 若 X_n \\xrightarrow{d} c，则其概率质量集中于 c 附近，即 \\Pr(|X_n-c|&gt;\\varepsilon)\\to 0；反之亦然。\n\n(2) 取有界连续函数 \\varphi，比较 \\E[\\varphi(Y_n)]-\\E[\\varphi(X)]，并拆为\n\\E[\\varphi(Y_n)-\\varphi(X_n)]+\\E[\\varphi(X_n)-\\varphi(X)]。\n前者由均匀连续性与 d(X_n,Y_n)\\xrightarrow{p}0 控制为 0，后者由 X_n\\xrightarrow{d}X 控制，故 Y_n\\xrightarrow{d}X。\n\n(3) 取二维连续函数 \\psi(x,y)，插入 \\psi(X_n,c)，结合 X_n\\xrightarrow{d}X 与 Y_n\\xrightarrow{p}c，即可得\n(X_n,Y_n)\\xrightarrow{d}(X,c)。\n\n\n\n\n\n由 连续映射定理 与 Slutsky 定理 可得以下常用结论：\n\nX_n + Y_n \\xrightarrow{d} X + c, \\quad\nX_n Y_n \\xrightarrow{d} cX.\n\n若 c \\neq 0，还可得到：\n\n\\frac{X_n}{Y_n} \\xrightarrow{d} \\frac{X}{c}."
  },
  {
    "objectID": "guides/probability/notes/slutsky.html#总结与启发",
    "href": "guides/probability/notes/slutsky.html#总结与启发",
    "title": "连续映射定理与 Slutsky 定理",
    "section": "",
    "text": "要点回顾：\n\n连续映射定理保证了“函数变换”下的收敛保持；\nSlutsky 定理则描述了“混合类型收敛”的稳定性；\n二者共同构成极限理论与渐近统计分析的技术基础。\n\n\n\n延伸阅读：\n📘 Probability and Measure — Billingsley, Ch. (Chapter 2–3)\n📗 Asymptotic Statistics — van der Vaart, A. W.\n📙 High-Dimensional Probability — Vershynin, R.\n推荐顺序：先掌握三类收敛，再阅读 Portmanteau 与 Slutsky 的证明。"
  },
  {
    "objectID": "guides/book/02-policy-gradient.html",
    "href": "guides/book/02-policy-gradient.html",
    "title": "Policy Gradient 其实没那么神秘",
    "section": "",
    "text": "我们定义回报 \nJ(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right].\n\n这不是玄学，这是优化目标。\n\n\n经典策略梯度定理给出： \n\\nabla_\\theta J(\\theta)\n= \\mathbb{E}_\\pi \\big[\n   \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, R\n\\big].\n\n\n\n\n“谁带来高回报，就让它以后更容易再次发生。”"
  },
  {
    "objectID": "guides/book/02-policy-gradient.html#梯度长什么样",
    "href": "guides/book/02-policy-gradient.html#梯度长什么样",
    "title": "Policy Gradient 其实没那么神秘",
    "section": "",
    "text": "经典策略梯度定理给出： \n\\nabla_\\theta J(\\theta)\n= \\mathbb{E}_\\pi \\big[\n   \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, R\n\\big]."
  },
  {
    "objectID": "guides/book/02-policy-gradient.html#直觉",
    "href": "guides/book/02-policy-gradient.html#直觉",
    "title": "Policy Gradient 其实没那么神秘",
    "section": "",
    "text": "“谁带来高回报，就让它以后更容易再次发生。”"
  },
  {
    "objectID": "guides/diffusion/index.html",
    "href": "guides/diffusion/index.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "学习目标\n从零到一理解扩散模型：动机 → 高斯扩散 → 反向 SDE/ODE → DDPM/DDIM → Flow Matching → 理论与实践。"
  },
  {
    "objectID": "guides/diffusion/index.html#扩散模型要解决的问题",
    "href": "guides/diffusion/index.html#扩散模型要解决的问题",
    "title": "Diffusion Models",
    "section": "扩散模型要解决的问题",
    "text": "扩散模型要解决的问题\n生成模型希望学习未知分布 p^*(x)，从而生成近似样本。\n直接从噪声“一步”生成复杂数据往往困难，而扩散模型的关键在于：\n\n前向扩散（Forward Diffusion）：逐步加噪，使数据分布趋近高斯；\n反向过程（Reverse Diffusion）：学习去噪映射，把噪声还原为数据；\n每一步都只需建模相邻分布 p(x_{t-\\Delta t}\\mid x_t)，学习更稳定。\n\n\n核心等价关系：\n\\text{Denoising} \\;\\Leftrightarrow\\; \\text{Learning } \\nabla_x \\log p_t(x)\n学习去噪即学习“得分函数”，方向即密度上升最快的方向。"
  },
  {
    "objectID": "guides/diffusion/index.html#模型概览",
    "href": "guides/diffusion/index.html#模型概览",
    "title": "Diffusion Models",
    "section": "模型概览",
    "text": "模型概览\n\n\n\n\n\n\n\n\n模型\n核心特征\n适合理解的方向\n\n\n\n\nDDPM (Ho et al., 2020)\n随机反演，采样多样性强\n适合学习随机微分方程与噪声调度思想\n\n\nDDIM (Song et al., 2020)\n确定性采样，速度快\n适合工程与推理效率研究\n\n\nFlow Matching (Lipman et al., 2023)\n统一连续流框架\n适合理解 ODE/SDE 统一与几何流"
  },
  {
    "objectID": "guides/diffusion/index.html#推荐网课与讲座",
    "href": "guides/diffusion/index.html#推荐网课与讲座",
    "title": "Diffusion Models",
    "section": "🎓 推荐网课与讲座",
    "text": "🎓 推荐网课与讲座\n\n\n\n课程\n来源\n定位与特点\n适合人群\n\n\n\n\nMIT 6.S980 — Diffusion Models\nMIT\n最系统的扩散入门课程；讲清从加噪→去噪→采样的完整流程，覆盖 DDPM、SDE、Flow Matching\n希望“系统掌握原理+数学背景”的研究生\n\n\nCS236 — Deep Generative Models\nStanford\n将扩散模型放在生成模型的整体框架中讲解（与VAE、GAN对比）\n想理解扩散在整个生成建模中的位置\n\n\nSTAT 157 — Deep Unsupervised Learning\nBerkeley\n从“概率建模 + 实验”的角度介绍 score-based 方法，重视动机和实践\n想边学边做、快速上手的同学\n\n\nDiffusion Seminar Series\nDeepMind × UCL\n汇集各大实验室研究者讲解前沿思想（Consistency、Flow Matching 等）\n已有基础，想了解学界前沿动向的人"
  },
  {
    "objectID": "guides/diffusion/index.html#推荐资料与书籍",
    "href": "guides/diffusion/index.html#推荐资料与书籍",
    "title": "Diffusion Models",
    "section": "推荐资料与书籍",
    "text": "推荐资料与书籍\n\n入门推荐：Step-by-Step Diffusion（arXiv:2406.08929）\n\n原文：https://arxiv.org/pdf/2406.08929\n\n这篇教程以最小数学依赖讲解扩散模型的核心直觉与采样流程，强调“加噪→去噪”的构造思路，帮助快速形成整体认知（DDPM、DDIM、Flow Matching 的联系与差异），在Appendix里整理了更深入的内容。\n谁适合读？ - 想先抓“为什么这样做”的直觉，而非从繁琐推导的数学理论开始；\n\n\n下方自动汇总所有 Diffusion 相关笔记（notes/*.qmd）\n在 guides/diffusion/notes/ 目录中添加新 .qmd 文件即可自动展示。"
  },
  {
    "objectID": "guides/diffusion/notes/ddpm.html",
    "href": "guides/diffusion/notes/ddpm.html",
    "title": "DDPM 基础推导",
    "section": "",
    "text": "简述：x_{t+\\Delta t}=x_t+\\eta_t,\\ \\eta_t\\sim\\mathcal N(0,\\sigma_q^2\\Delta t)。小步噪声下， p(x_{t-\\Delta t}\\mid x_t)\\approx \\mathcal N(\\mu_t(x_t),\\sigma_q^2\\Delta t)， 回归目标 \\arg\\min_f \\mathbb E\\|f(x_t)-x_{t-\\Delta t}\\|^2。"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Paper Picks",
    "section": "",
    "text": "高信号论文精选：关注 ML Theory / RL / Optimization / Generative Models。\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nauthors\n\n\n\nvenue\n\n\n\nyear\n\n\n\ntags\n\n\n\n\n\n\n\n\nSelf-Concordant Barriers Revisited\n\n\nNesterov, Nemirovski\n\n\nMath. Programming\n\n\n1994\n\n\noptimization, self-concordant, interior-point\n\n\n\n\n\n\nNo matching items"
  }
]