[
  {
    "objectID": "papers/2025-10-08-paper-template.html",
    "href": "papers/2025-10-08-paper-template.html",
    "title": "Self-Concordant Barriers Revisited",
    "section": "",
    "text": "Tip\n\n\n\nTL;DR 以自洽函数为桥梁，统一了内点法的复杂度与几何直觉。"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#key-ideas",
    "href": "papers/2025-10-08-paper-template.html#key-ideas",
    "title": "Self-Concordant Barriers Revisited",
    "section": "Key Ideas",
    "text": "Key Ideas\n\nBarrier 的自洽性定义与局部二次近似…\nNewton decrement 与路径跟随…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#why-it-matters",
    "href": "papers/2025-10-08-paper-template.html#why-it-matters",
    "title": "Self-Concordant Barriers Revisited",
    "section": "Why it matters",
    "text": "Why it matters\n\n将凸优化的复杂度理论从“算法技巧”上升到“几何结构”…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#my-notes",
    "href": "papers/2025-10-08-paper-template.html#my-notes",
    "title": "Self-Concordant Barriers Revisited",
    "section": "My notes",
    "text": "My notes\n\n直观图示…\n延伸阅读：…"
  },
  {
    "objectID": "papers/2025-10-08-paper-template.html#bibtex",
    "href": "papers/2025-10-08-paper-template.html#bibtex",
    "title": "Self-Concordant Barriers Revisited",
    "section": "BibTeX",
    "text": "BibTeX\n```bibtex @article{nesterov1994interior, … }"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PhD student / Researcher in Shanghai\nInterests: Probability, Machine Learning, Diffusion Models\nI write about ideas-in-progress — learning in public."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "PhD student / Researcher in Shanghai\nInterests: Probability, Machine Learning, Diffusion Models\nI write about ideas-in-progress — learning in public."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n\nGitHub: https://github.com/MIStatlE"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html",
    "href": "guides/diffusion/notes/vae.html",
    "title": "VAE 基础推导",
    "section": "",
    "text": "我们希望估计某个分布 p(x)，用参数化模型 p_\\theta(x) 来近似，并通过极大似然估计优化参数：\n\n\\max_\\theta \\ \\mathbb{E}_{x\\sim p^*}[\\log p_\\theta(x)].\n\n但在高维空间上直接建模 p_\\theta(x) 通常非常困难，因此我们引入潜变量 z，将复杂分布分解为条件形式：\n\np_\\theta(x)=\\int p_\\theta(x|z)p(z)\\,dz,\n\n其中 p_\\theta(x|z) 称为 decoder（生成器），p(z) 是先验（通常取标准正态）。\n\n\n\n\n真实的后验 p_\\theta(z|x) 无法直接计算，于是定义可计算的近似分布：\n\nq_\\phi(z|x) \\approx p_\\theta(z|x),\n\n称为 encoder（编码器）。\n两者通常使用神经网络参数化：\n\nq_\\phi(z|x)=\\mathcal N(z|\\mu_\\phi(x),\\mathrm{diag}(\\sigma_\\phi^2(x))), \\quad\np_\\theta(x|z)=\\mathcal N(x|f_\\theta(z),\\sigma_{\\text{dec}}^2 I).\n\n\n\n\n\n直接优化 \\log p_\\theta(x) 不可行，因为积分项难计算：\n\np_\\theta(x)=\\int p_\\theta(x,z)\\,dz.\n\n于是我们引入 q_\\phi(z|x) 并重写：\n\n\\log p_\\theta(x)\n= \\log\\int q_\\phi(z|x)\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\,dz\n= \\log \\mathbb{E}_{q_\\phi}\\!\\left[\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right].\n\n应用 Jensen 不等式（\\log \\mathbb{E}[\\cdot] \\ge \\mathbb{E}[\\log\\cdot]）得到：\n\n\\log p_\\theta(x)\n\\ge \\mathbb{E}_{q_\\phi}\\!\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\n=: \\mathcal L(\\theta,\\phi;x),\n\n这就是著名的 Evidence Lower Bound (ELBO)。\n\n\n\n\n展开 p_\\theta(x,z)=p_\\theta(x|z)p(z)，有：\n\n\\mathcal L(\\theta,\\phi;x)\n= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\n- \\mathrm{KL}(q_\\phi(z|x)\\|p(z)).\n\n\n重建项：衡量生成器 p_\\theta(x|z) 对输入 x 的复现能力；\n正则项：约束编码器 q_\\phi(z|x) 贴近先验 p(z)，防止潜变量退化。\n\n等价关系：\n\n\\log p_\\theta(x)\n= \\mathcal L(\\theta,\\phi;x)\n+ \\mathrm{KL}(q_\\phi(z|x)\\|p_\\theta(z|x)),\n\n因此 ELBO 始终是 \\log p_\\theta(x) 的下界。\n\n推荐阅读：\n📖 arXiv:2406.08929 —— 一份以直觉为主的 Diffusion 入门材料。\n先建立整体框架，再看推导细节，会更高效。\n#生成模型 #VAE #Diffusion #变分推断"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#从极大似然出发",
    "href": "guides/diffusion/notes/vae.html#从极大似然出发",
    "title": "VAE 基础推导",
    "section": "",
    "text": "我们希望估计某个分布 p(x)，用参数化模型 p_\\theta(x) 来近似，并通过极大似然估计优化参数：\n\n\\max_\\theta \\ \\mathbb{E}_{x\\sim p^*}[\\log p_\\theta(x)].\n\n但在高维空间上直接建模 p_\\theta(x) 通常非常困难，因此我们引入潜变量 z，将复杂分布分解为条件形式：\n\np_\\theta(x)=\\int p_\\theta(x|z)p(z)\\,dz,\n\n其中 p_\\theta(x|z) 称为 decoder（生成器），p(z) 是先验（通常取标准正态）。"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#引入可计算的近似分布",
    "href": "guides/diffusion/notes/vae.html#引入可计算的近似分布",
    "title": "VAE 基础推导",
    "section": "",
    "text": "真实的后验 p_\\theta(z|x) 无法直接计算，于是定义可计算的近似分布：\n\nq_\\phi(z|x) \\approx p_\\theta(z|x),\n\n称为 encoder（编码器）。\n两者通常使用神经网络参数化：\n\nq_\\phi(z|x)=\\mathcal N(z|\\mu_\\phi(x),\\mathrm{diag}(\\sigma_\\phi^2(x))), \\quad\np_\\theta(x|z)=\\mathcal N(x|f_\\theta(z),\\sigma_{\\text{dec}}^2 I)."
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#从最大似然到-elbo",
    "href": "guides/diffusion/notes/vae.html#从最大似然到-elbo",
    "title": "VAE 基础推导",
    "section": "",
    "text": "直接优化 \\log p_\\theta(x) 不可行，因为积分项难计算：\n\np_\\theta(x)=\\int p_\\theta(x,z)\\,dz.\n\n于是我们引入 q_\\phi(z|x) 并重写：\n\n\\log p_\\theta(x)\n= \\log\\int q_\\phi(z|x)\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\,dz\n= \\log \\mathbb{E}_{q_\\phi}\\!\\left[\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right].\n\n应用 Jensen 不等式（\\log \\mathbb{E}[\\cdot] \\ge \\mathbb{E}[\\log\\cdot]）得到：\n\n\\log p_\\theta(x)\n\\ge \\mathbb{E}_{q_\\phi}\\!\\left[\\log\\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\right]\n=: \\mathcal L(\\theta,\\phi;x),\n\n这就是著名的 Evidence Lower Bound (ELBO)。"
  },
  {
    "objectID": "guides/diffusion/notes/vae.html#elbo-的结构与意义",
    "href": "guides/diffusion/notes/vae.html#elbo-的结构与意义",
    "title": "VAE 基础推导",
    "section": "",
    "text": "展开 p_\\theta(x,z)=p_\\theta(x|z)p(z)，有：\n\n\\mathcal L(\\theta,\\phi;x)\n= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\n- \\mathrm{KL}(q_\\phi(z|x)\\|p(z)).\n\n\n重建项：衡量生成器 p_\\theta(x|z) 对输入 x 的复现能力；\n正则项：约束编码器 q_\\phi(z|x) 贴近先验 p(z)，防止潜变量退化。\n\n等价关系：\n\n\\log p_\\theta(x)\n= \\mathcal L(\\theta,\\phi;x)\n+ \\mathrm{KL}(q_\\phi(z|x)\\|p_\\theta(z|x)),\n\n因此 ELBO 始终是 \\log p_\\theta(x) 的下界。\n\n推荐阅读：\n📖 arXiv:2406.08929 —— 一份以直觉为主的 Diffusion 入门材料。\n先建立整体框架，再看推导细节，会更高效。\n#生成模型 #VAE #Diffusion #变分推断"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides Overview",
    "section": "",
    "text": "Guides Overview\nStart with Probability & Stats or Optimization.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "tools/quarto-setup.html",
    "href": "tools/quarto-setup.html",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "",
    "text": "Markdown+数学+列表页+RSS，一体化…\n适合“利他型”学习空间：低摩擦、高输出。"
  },
  {
    "objectID": "tools/quarto-setup.html#why-quarto",
    "href": "tools/quarto-setup.html#why-quarto",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "",
    "text": "Markdown+数学+列表页+RSS，一体化…\n适合“利他型”学习空间：低摩擦、高输出。"
  },
  {
    "objectID": "tools/quarto-setup.html#setup",
    "href": "tools/quarto-setup.html#setup",
    "title": "Quarto 最佳实践：从 0 到“高信息密度”站点",
    "section": "Setup",
    "text": "Setup\n```bash quarto create-project MIStatlE-Lab quarto add quarto-ext/fontawesome"
  },
  {
    "objectID": "posts/2025-10-07-welcome.html",
    "href": "posts/2025-10-07-welcome.html",
    "title": "Starting Over: Building MIStatlE Lab",
    "section": "",
    "text": "This is a starter post. You can write math like e^{i\\pi}+1=0 and code blocks:\ndef add(a, b): return a + b\nprint(add(2, 3))\nTo publish, just push to main — GitHub Actions will render and deploy to gh-pages."
  },
  {
    "objectID": "search.html",
    "href": "search.html",
    "title": "FRAIMEL",
    "section": "",
    "text": "Tip\n\n\n\nUse the navbar search (browser) or your site search engine."
  },
  {
    "objectID": "search.html#title-search",
    "href": "search.html#title-search",
    "title": "FRAIMEL",
    "section": "",
    "text": "Tip\n\n\n\nUse the navbar search (browser) or your site search engine."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hello, I’m MIStatlE 👋\nThis is my minimal Quarto website. Math works out of the box:\n\nInline math: \\int_0^1 x^2\\,dx = 1/3\nDisplay math:\n\n\n\\mathbb{P}(|S_n - n\\mu| \\ge t) \\le 2\\exp\\!\\left(-\\frac{t^2}{2\\sigma^2 n}\\right).\n\nUse the Posts section for articles and notes. Edit _quarto.yml to customize the navbar and theme."
  },
  {
    "objectID": "reads/index.html",
    "href": "reads/index.html",
    "title": "Good Reads",
    "section": "",
    "text": "高信息密度外部资源：长文、书单、课程、视频与博客。\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "community/index.html",
    "href": "community/index.html",
    "title": "Community",
    "section": "",
    "text": "Community\nEvents, announcements, and resources."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Welcome to my posts.\n\n\n\n\n\n\n\n\n\nStarting Over: Building MIStatlE Lab\n\n\n\nmeta\n\nsetup\n\n\n\nA simple Quarto starter with math support, post listings, and GitHub Pages auto-deploy.\n\n\n\n\n\nOct 7, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Tools & Workflows",
    "section": "",
    "text": "可复用的研究/工程工具与工作流，强调 可操作 与 效率。\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nQuarto 最佳实践：从 0 到“高信息密度”站点\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/probability/index.html",
    "href": "guides/probability/index.html",
    "title": "Probability Theory",
    "section": "",
    "text": "学习目标\n从直觉出发构建现代概率论体系：随机变量 → 收敛与极限定理 → 大数与中心极限定理 → 集中不等式 → 随机过程。"
  },
  {
    "objectID": "guides/probability/index.html#概率论的核心问题",
    "href": "guides/probability/index.html#概率论的核心问题",
    "title": "Probability Theory",
    "section": "概率论的核心问题",
    "text": "概率论的核心问题\n概率论的目标是：刻画不确定性，并在随机性中寻找规律。\n在现代数学体系中，概率论不仅是统计推断的基础，更是机器学习理论、信息论、随机优化等领域的核心语言。\n它回答三个根本问题：\n\n随机性如何被形式化？\n—— 通过 (\\Omega, \\mathcal{F}, \\mathbb{P}) 概率空间定义不确定事件。\n如何度量“趋近确定”的过程？\n—— 研究不同类型的收敛（几乎处处、依概率、分布收敛）。\n大量随机样本的规律是什么？\n—— 大数定律与中心极限定理揭示了“稳定性”的来源。\n\n\n核心思想：\n\\text{Randomness} \\;\\Rightarrow\\; \\text{Structure via Law of Large Numbers}\n随机中的秩序，是现代概率论的灵魂。"
  },
  {
    "objectID": "guides/probability/index.html#模块概览",
    "href": "guides/probability/index.html#模块概览",
    "title": "Probability Theory",
    "section": "模块概览",
    "text": "模块概览\n\n\n\n\n\n\n\n\n模块\n内容要点\n理解重点\n\n\n\n\n测度化概率基础\n概率空间、可测函数、Lebesgue 积分\n把“事件”和“期望”放在统一框架中\n\n\n随机变量收敛\na.s., p, d 三类收敛与关系\n收敛的层次结构与连续映射定理\n\n\n极限定理体系\n大数定律、中心极限定理、Borel–Cantelli\n从样本到总体的统计稳定性\n\n\n集中不等式\nHoeffding, Bernstein, McDiarmid\n概率尾界与泛化分析的核心工具\n\n\n随机过程\nMartingale, Markov, Brownian Motion\n时序依赖下的不确定性分析"
  },
  {
    "objectID": "guides/probability/index.html#推荐课程与讲座",
    "href": "guides/probability/index.html#推荐课程与讲座",
    "title": "Probability Theory",
    "section": "🎓 推荐课程与讲座",
    "text": "🎓 推荐课程与讲座\n\n\n\n课程\n来源\n特点\n适合人群\n\n\n\n\nMIT 18.600 — Probability and Random Variables\nMIT OCW\n逻辑清晰、涵盖经典概率框架\n想系统掌握概率与分布理论的本科/研究生\n\n\nStanford STATS 310 — Probability Theory\nStanford\n以严谨分析为主，覆盖测度化概率\n想理解概率论数学基础与推导\n\n\nUCLA Math 170E/F — Probability and Stochastic Processes\nUCLA\n理论与应用结合，包含马尔可夫过程\n想了解随机过程在 ML 中的应用\n\n\nBerkeley STAT 205A/B\nBerkeley\n渐近与集中理论并重\n希望进阶理论与集中不等式的学生"
  },
  {
    "objectID": "guides/probability/index.html#推荐书籍与资料",
    "href": "guides/probability/index.html#推荐书籍与资料",
    "title": "Probability Theory",
    "section": "📚 推荐书籍与资料",
    "text": "📚 推荐书籍与资料\n\n入门与直觉\n\n《Introduction to Probability》 — Dimitri Bertsekas & John Tsitsiklis\n工程视角下的概率入门，直觉性强。\n《Probability and Random Processes》 — Grimmett & Stirzaker\n理论与推导兼顾，经典教材。\n\n\n\n理论与严谨\n\n《Probability: Theory and Examples》 — Rick Durrett\n高级概率教材，深入大数定律与马尔可夫链。\n《A Course in Probability Theory》 — Kai Lai Chung\n经典而简洁，适合建立测度化概率框架。\n《High-Dimensional Probability》 — Roman Vershynin\n现代视角下的集中不等式与随机矩阵分析。"
  },
  {
    "objectID": "guides/probability/index.html#学习建议",
    "href": "guides/probability/index.html#学习建议",
    "title": "Probability Theory",
    "section": "📈 学习建议",
    "text": "📈 学习建议\n\n学习路径建议：\n\n\n从有限概率与独立性开始（如骰子、抽签模型）；\n学习随机变量与期望的测度定义；\n掌握各种收敛的区别与联系；\n逐步引入极限定理（LLN、CLT）；\n学会使用集中不等式分析学习算法；\n进阶阅读 Martingale 与随机过程。\n\n\n\n自动汇总笔记\n本页面将自动展示所有位于 notes/*.qmd 下的概率论笔记。\n在 guides/probability/notes/ 文件夹中新增 .qmd 文件即可自动更新列表。"
  },
  {
    "objectID": "guides/diffusion/index.html",
    "href": "guides/diffusion/index.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "学习目标\n从零到一理解扩散模型：动机 → 高斯扩散 → 反向 SDE/ODE → DDPM/DDIM → Flow Matching → 理论与实践。"
  },
  {
    "objectID": "guides/diffusion/index.html#扩散模型要解决的问题",
    "href": "guides/diffusion/index.html#扩散模型要解决的问题",
    "title": "Diffusion Models",
    "section": "扩散模型要解决的问题",
    "text": "扩散模型要解决的问题\n生成模型希望学习未知分布 p^*(x)，从而生成近似样本。\n直接从噪声“一步”生成复杂数据往往困难，而扩散模型的关键在于：\n\n前向扩散（Forward Diffusion）：逐步加噪，使数据分布趋近高斯；\n反向过程（Reverse Diffusion）：学习去噪映射，把噪声还原为数据；\n每一步都只需建模相邻分布 p(x_{t-\\Delta t}\\mid x_t)，学习更稳定。\n\n\n核心等价关系：\n\\text{Denoising} \\;\\Leftrightarrow\\; \\text{Learning } \\nabla_x \\log p_t(x)\n学习去噪即学习“得分函数”，方向即密度上升最快的方向。"
  },
  {
    "objectID": "guides/diffusion/index.html#模型概览",
    "href": "guides/diffusion/index.html#模型概览",
    "title": "Diffusion Models",
    "section": "模型概览",
    "text": "模型概览\n\n\n\n\n\n\n\n\n模型\n核心特征\n适合理解的方向\n\n\n\n\nDDPM (Ho et al., 2020)\n随机反演，采样多样性强\n适合学习随机微分方程与噪声调度思想\n\n\nDDIM (Song et al., 2020)\n确定性采样，速度快\n适合工程与推理效率研究\n\n\nFlow Matching (Lipman et al., 2023)\n统一连续流框架\n适合理解 ODE/SDE 统一与几何流"
  },
  {
    "objectID": "guides/diffusion/index.html#推荐网课与讲座",
    "href": "guides/diffusion/index.html#推荐网课与讲座",
    "title": "Diffusion Models",
    "section": "🎓 推荐网课与讲座",
    "text": "🎓 推荐网课与讲座\n\n\n\n课程\n来源\n定位与特点\n适合人群\n\n\n\n\nMIT 6.S980 — Diffusion Models\nMIT\n最系统的扩散入门课程；讲清从加噪→去噪→采样的完整流程，覆盖 DDPM、SDE、Flow Matching\n希望“系统掌握原理+数学背景”的研究生\n\n\nCS236 — Deep Generative Models\nStanford\n将扩散模型放在生成模型的整体框架中讲解（与VAE、GAN对比）\n想理解扩散在整个生成建模中的位置\n\n\nSTAT 157 — Deep Unsupervised Learning\nBerkeley\n从“概率建模 + 实验”的角度介绍 score-based 方法，重视动机和实践\n想边学边做、快速上手的同学\n\n\nDiffusion Seminar Series\nDeepMind × UCL\n汇集各大实验室研究者讲解前沿思想（Consistency、Flow Matching 等）\n已有基础，想了解学界前沿动向的人"
  },
  {
    "objectID": "guides/diffusion/index.html#推荐资料与书籍",
    "href": "guides/diffusion/index.html#推荐资料与书籍",
    "title": "Diffusion Models",
    "section": "推荐资料与书籍",
    "text": "推荐资料与书籍\n\n入门推荐：Step-by-Step Diffusion（arXiv:2406.08929）\n\n原文：https://arxiv.org/pdf/2406.08929\n\n这篇教程以最小数学依赖讲解扩散模型的核心直觉与采样流程，强调“加噪→去噪”的构造思路，帮助快速形成整体认知（DDPM、DDIM、Flow Matching 的联系与差异），在Appendix里整理了更深入的内容。\n谁适合读？ - 想先抓“为什么这样做”的直觉，而非从繁琐推导的数学理论开始；\n\n\n下方自动汇总所有 Diffusion 相关笔记（notes/*.qmd）\n在 guides/diffusion/notes/ 目录中添加新 .qmd 文件即可自动展示。"
  },
  {
    "objectID": "guides/diffusion/notes/ddpm.html",
    "href": "guides/diffusion/notes/ddpm.html",
    "title": "DDPM 基础推导",
    "section": "",
    "text": "简述：x_{t+\\Delta t}=x_t+\\eta_t,\\ \\eta_t\\sim\\mathcal N(0,\\sigma_q^2\\Delta t)。小步噪声下， p(x_{t-\\Delta t}\\mid x_t)\\approx \\mathcal N(\\mu_t(x_t),\\sigma_q^2\\Delta t)， 回归目标 \\arg\\min_f \\mathbb E\\|f(x_t)-x_{t-\\Delta t}\\|^2。"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Paper Picks",
    "section": "",
    "text": "高信号论文精选：关注 ML Theory / RL / Optimization / Generative Models。\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nauthors\n\n\n\nvenue\n\n\n\nyear\n\n\n\ntags\n\n\n\n\n\n\n\n\nSelf-Concordant Barriers Revisited\n\n\nNesterov, Nemirovski\n\n\nMath. Programming\n\n\n1994\n\n\noptimization, self-concordant, interior-point\n\n\n\n\n\n\nNo matching items"
  }
]